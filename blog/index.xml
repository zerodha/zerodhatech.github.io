<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blogs on Zerodha Tech Blog</title><link>https://zerodha.tech/blog/</link><description>Recent content in Blogs on Zerodha Tech Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 14 Feb 2024 00:00:00 +0530</lastBuildDate><atom:link href="https://zerodha.tech/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>1.5+ million PDFs in 25 minutes</title><link>https://zerodha.tech/blog/1-5-million-pdfs-in-25-minutes/</link><pubDate>Wed, 14 Feb 2024 00:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/1-5-million-pdfs-in-25-minutes/</guid><description>At Zerodha, many million users login and use our financial platforms every day. Over the recent months, on an average day, 1.5+ million users have been executing stock and derivative transactions. On a volatile day, this number could easily double. After a trading session concludes and all the number-crunching, tallying, and “backoffice” operations are completed—with file dumps received from stock exchanges and other market infrastructure institutions—stock brokers e-mail a digitally signed PDF report called the contract note to every user who transacted on that particular day.</description></item><item><title>Logging at Zerodha</title><link>https://zerodha.tech/blog/logging-at-zerodha/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/logging-at-zerodha/</guid><description>At Zerodha, we run a multitude of internal and public-facing services that generate copious amounts of logs. While developers use these logs to debug or troubleshoot incidents, some services also emit logs that must be persisted for prolonged periods to comply with numerous regulatory requirements. In this post, I will delve into our experiences with the ELK stack, why it didn&amp;rsquo;t fit our needs and our migration to ClickHouse.
Why ELK wasn’t the right fit for us In 2018, we adopted the ELK stack as our de facto stack for storing application logs.</description></item><item><title>User disengagement</title><link>https://zerodha.tech/blog/user-disengagement/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://zerodha.tech/blog/user-disengagement/</guid><description>Over time, I have come to realise that the term user &amp;ldquo;engagement&amp;rdquo; in software, more often than not, is a thinly veiled proxy for user entrapment, whether intentional on the part of software designers or not. The trend of shallow user numbers and &amp;ldquo;engagement&amp;rdquo; metrics taking centre stage in business valuations has created perverse incentives at a massive scale for software to indulge in unethical practices of &amp;ldquo;engaging&amp;rdquo; users at any cost, with no respect for the limited and fast eroding levels of cognitive resources.</description></item><item><title>From Native to React Native to Flutter</title><link>https://zerodha.tech/blog/from-native-to-react-native-to-flutter/</link><pubDate>Mon, 17 Jan 2022 16:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/from-native-to-react-native-to-flutter/</guid><description>At Zerodha, the first mobile version our flagship trading platform Kite was written as a native Android app in 2015. After building a cross-platform version in React Native in 2017, we finally settled for a full rewrite in Flutter in 2018, a choice that has paid off really well for us. There were several factors and trade-offs that prompted these rewrites.
This post covers our journey and experiences with each of the frameworks, and why we finally bet on Flutter even when it was bleeding edge alpha technology.</description></item><item><title>Scaling with common sense #2: Being future ready.</title><link>https://zerodha.tech/blog/being-future-ready-with-common-sense/</link><pubDate>Wed, 28 Jul 2021 15:30:00 +0530</pubDate><guid>https://zerodha.tech/blog/being-future-ready-with-common-sense/</guid><description>Over the last year, owing to the unexpected rally in capital markets, Zerodha&amp;rsquo;s customer base has more than tripled, significantly increasing the number of concurrent users on our platforms along with the traffic and load they generate on numerous systems in the background. For context, in January 2020, we were handling 2+ million retail trades daily. In April 2020, it had risen to 7+ million. Today, it goes up to 12+ million.</description></item><item><title>Working with PostgreSQL</title><link>https://zerodha.tech/blog/working-with-postgresql/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/working-with-postgresql/</guid><description>This post is in the context of the large, data heavy PostgreSQL instances that store historical transactional data and reports, the databases that power Console and its large scale number crunching and reporting. It talks about how we self-host, tune, and manage all our DB instances on bare EC2 instances. For high availability and backups, we use simple failover replicas and for backups, AWS disk snapshots.
The Console DBs store hundreds of billions of rows of different kinds of financial and transactional data, currently close to 20 TB, across four sharded nodes.</description></item><item><title>A lesson in creating and using niche business DSLs at scale</title><link>https://zerodha.tech/blog/a-lesson-in-niche-business-dsls-at-scale/</link><pubDate>Tue, 24 Nov 2020 17:35:00 +0530</pubDate><guid>https://zerodha.tech/blog/a-lesson-in-niche-business-dsls-at-scale/</guid><description>At Zerodha, we process millions of trades in real-time, where each trade comes into the system as concurrent high throughput HTTP requests. Each trade increases the latency for subsequent orders in the queue that are under processing at the same time at our OMS (Order Management System). When a single order comes through to the OMS, it goes through a bunch of computationally intensive validations and adds to the latency. To reduce the latency of orders, we decided to offload some of these business validations from the OMS into an external component called Veto, which pre-validates incoming orders based on custom dynamic rules set by our Risk Management team.</description></item><item><title>Alar: The making of an open source dictionary</title><link>https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/</link><pubDate>Tue, 22 Sep 2020 00:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/</guid><description>ನಮಸ್ಕಾರ (Namaskāra)! This is not a post on fintech, or even technology for that matter. This is the story of a product of tenacity, selflessness, and passion; a product that will transcend and outlive most technology we know of. This is the story of a massive dictionary that will become the window to a language spoken by tens of millions of people for generations to come, a resource its author has donated to posterity.</description></item><item><title>Scaling with common sense</title><link>https://zerodha.tech/blog/scaling-with-common-sense/</link><pubDate>Sun, 14 Jun 2020 14:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/scaling-with-common-sense/</guid><description>&amp;ldquo;Scaling&amp;rdquo; is not a specific technique but an entire spectrum that stretches all the way from &amp;ldquo;Google scale&amp;rdquo; to the K8s cluster Karan runs on a Raspberry Pi plugged into a 12V shaver outlet in his bathroom to encrypt his DNS queries for peak privacy, while he asks Alexa to dim the lights and play his favorite non-mainstream indie music.
It is a collection of practices unique and intimate to every organisation, and is the product of an infinite number of variables; the numerous domain-specific problems, the structure of the organisation, the nature of the people involved and their biases, countless engineering decisions and trade-offs, technical debt and history, ad infinitum.</description></item><item><title>Infrastructure monitoring with Prometheus at Zerodha</title><link>https://zerodha.tech/blog/infra-monitoring-at-zerodha/</link><pubDate>Mon, 27 Apr 2020 00:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/infra-monitoring-at-zerodha/</guid><description>On any given day, we handle around 15% of daily retail trading volume across all stock exchanges in India. Billions of requests generated in the process are handled by a suite of systems we have built in-house. Also, we are very particular on self-hosting as many dependencies as possible, everything from CRMs to large databases, Kafka clusters, mail servers etc.
To aid these primary systems, there are a large number of ancillary workloads that run, covering everything from real-time trades, document processing, KYC, and account opening, legal and compliance, complex, large scale P&amp;amp;L and number crunching, and a wide range of backoffice workloads.</description></item><item><title>Hello, World!</title><link>https://zerodha.tech/blog/hello-world/</link><pubDate>Mon, 06 Apr 2020 00:00:00 +0530</pubDate><guid>https://zerodha.tech/blog/hello-world/</guid><description>Zerodha, now India&amp;rsquo;s largest stock broker, bootstrapped and profitable, turns ten years old this year. The Zerodha tech team turns seven years old. The tech team has remained largely elusive over the course of our existence. While we have pondered starting a tech blog for more than half a decade, we have often found ourselves too busy building the fundamental blocks underlying a stock brokerage. We have also been stalled by a sense of unpreparedness to talk to the world about our very unconventional setup.</description></item></channel></rss>